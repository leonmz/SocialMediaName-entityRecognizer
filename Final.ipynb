{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "\n",
    "def warning(msg):\n",
    "    print (\"WARNING:\", msg)\n",
    "\n",
    "def convert_bio_to_spans(bio_sequence):\n",
    "    spans = []  # (label, startindex, endindex)\n",
    "    cur_start = None\n",
    "    cur_label = None\n",
    "    N = len(bio_sequence)\n",
    "    for t in range(N+1):\n",
    "        if ((cur_start is not None) and\n",
    "                (t==N or re.search(\"^[BO]\", bio_sequence[t]))):\n",
    "            assert cur_label is not None\n",
    "            spans.append((cur_label, cur_start, t))\n",
    "            cur_start = None\n",
    "            cur_label = None\n",
    "        if t==N: continue\n",
    "        assert bio_sequence[t] and bio_sequence[t][0] in (\"B\",\"I\",\"O\")\n",
    "        if bio_sequence[t].startswith(\"B\"):\n",
    "            cur_start = t\n",
    "            cur_label = re.sub(\"^B-?\",\"\", bio_sequence[t]).strip()\n",
    "        if bio_sequence[t].startswith(\"I\"):\n",
    "            if cur_start is None:\n",
    "                warning(\"BIO inconsistency: I without starting B. Rewriting to B.\")\n",
    "                newseq = bio_sequence[:]\n",
    "                newseq[t] = \"B\" + newseq[t][1:]\n",
    "                return convert_bio_to_spans(newseq)\n",
    "            continuation_label = re.sub(\"^I-?\",\"\",bio_sequence[t])\n",
    "            if continuation_label != cur_label:\n",
    "                newseq = bio_sequence[:]\n",
    "                newseq[t] = \"B\" + newseq[t][1:]\n",
    "                warning(\"BIO inconsistency: %s but current label is '%s'. Rewriting to %s\" % (bio_sequence[t], cur_label, newseq[t]))\n",
    "                return convert_bio_to_spans(newseq)\n",
    "\n",
    "    # should have exited for last span ending at end by now\n",
    "    assert cur_start is None\n",
    "    spancheck(spans)\n",
    "    return spans\n",
    "\n",
    "def test_bio_conversion():\n",
    "    spans = convert_bio_to_spans([\"B\"])\n",
    "    assert spans==[(\"\",0,1)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\",\"O\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"O\",\"B\",\"I\",\"O\",\"O\"])\n",
    "    assert spans==[(\"\",1,3)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"B\"])\n",
    "    assert spans==[(\"\",0,1), (\"\",1,2)]\n",
    "    spans = convert_bio_to_spans([\"B\",\"I\",\"B\"])\n",
    "    assert spans==[(\"\",0,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"B-asdf\",\"I-asdf\",\"B\"])\n",
    "    assert spans==[(\"asdf\",0,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"B-asdf\",\"I-difftype\",\"B\"])\n",
    "    assert spans==[(\"asdf\",0,1), (\"difftype\",1,2), (\"\",2,3)]\n",
    "    spans = convert_bio_to_spans([\"I\",\"I\"])\n",
    "    assert spans==[(\"\",0,2)]\n",
    "    spans = convert_bio_to_spans([\"B-a\",\"I-b\"])\n",
    "    assert spans==[(\"a\",0,1), (\"b\",1,2)]\n",
    "\n",
    "\n",
    "def spancheck(spanlist):\n",
    "    s = set(spanlist)\n",
    "    assert len(s)==len(spanlist), \"spans are non-unique ... is this a bug in the eval script?\"\n",
    "\n",
    "def kill_labels(bio_seq):\n",
    "    ret = []\n",
    "    for x in bio_seq:\n",
    "        if re.search(\"^[BI]\", x):\n",
    "            x = re.sub(\"^B.*\",\"B\", x)\n",
    "            x = re.sub(\"^I.*\",\"I\", x)\n",
    "        ret.append(x)\n",
    "    return ret\n",
    "\n",
    "def evaluate_taggings(goldseq_predseq_pairs, ignore_labels=False):\n",
    "    \"\"\"a list of (goldtags,predtags) pairs.  goldtags and predtags are both lists of strings, of the same length.\"\"\"\n",
    "    num_sent = 0\n",
    "    num_tokens= 0\n",
    "    num_goldspans = 0\n",
    "    num_predspans = 0\n",
    "\n",
    "    tp, fp, fn = 0,0,0\n",
    "\n",
    "    for goldseq,predseq in goldseq_predseq_pairs:\n",
    "        N = len(goldseq)\n",
    "        assert N==len(predseq)\n",
    "        num_sent += 1\n",
    "        num_tokens += N\n",
    "\n",
    "        if ignore_labels:\n",
    "            goldseq = kill_labels(goldseq)\n",
    "            predseq = kill_labels(predseq)\n",
    "\n",
    "        goldspans = convert_bio_to_spans(goldseq)\n",
    "        predspans = convert_bio_to_spans(predseq)\n",
    "\n",
    "        num_goldspans += len(goldspans)\n",
    "        num_predspans += len(predspans)\n",
    "\n",
    "        goldspans_set = set(goldspans)\n",
    "        predspans_set = set(predspans)\n",
    "\n",
    "        # tp: number of spans that gold and pred have\n",
    "        # fp: number of spans that pred had that gold didn't (incorrect predictions)\n",
    "        # fn: number of spans that gold had that pred didn't (didn't recall)\n",
    "        tp += len(goldspans_set & predspans_set)\n",
    "        fp += len(predspans_set - goldspans_set)\n",
    "        fn += len(goldspans_set - predspans_set)\n",
    "\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 1\n",
    "    rec =  tp/(tp+fn) if (tp+fn)>0 else 1\n",
    "    f1 = 2*prec*rec / (prec + rec)\n",
    "    print(\"F = {f1:.4f},  Prec = {prec:.4f} ({tp}/{tpfp}),  Rec = {rec:.4f} ({tp}/{tpfn})\".format(\n",
    "            tpfp=tp+fp, tpfn=tp+fn, **locals()))\n",
    "    print(\"({num_sent} sentences, {num_tokens} tokens, {num_goldspans} gold spans, {num_predspans} predicted spans)\".format(**locals()))\n",
    "    return f1\n",
    "\n",
    "def read_tokens_tags_file(filename):\n",
    "    \"\"\"Returns list of sentences.  each sentence is a pair (tokens, tags), each\n",
    "    of which is a list of strings of the same length.\"\"\"\n",
    "    sentences = open(filename).read().strip().split(\"\\n\\n\")\n",
    "    ret = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        lines = sent.split(\"\\n\")\n",
    "        pairs = [L.split(\"\\t\") for L in lines]\n",
    "        for pair in pairs:\n",
    "            assert len(pair)==2, \"Was expecting 2 tab-separated items per line.\"\n",
    "        tokens = [tok for tok,tag in pairs]\n",
    "        tags = [tag for tok,tag in pairs]\n",
    "        ret.append( (tokens,tags) )\n",
    "    return ret\n",
    "\n",
    "def read_tags_file(filename):\n",
    "    sentences = open(filename).read().strip().split(\"\\n\\n\")\n",
    "    ret = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        lines = sent.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            assert len(line.split())==1, \"Was expecting 1 item per line\"\n",
    "        ret.append( [line.strip() for line in lines] )\n",
    "    return ret\n",
    "\n",
    "def evaluate_tagging_file(gold_tags_file, predicted_tags_file):\n",
    "    tokens_and_tags = read_tokens_tags_file(gold_tags_file)\n",
    "    goldseqs = [tags for tokens,tags in tokens_and_tags]\n",
    "\n",
    "    # assume predicted_tags_file is the simple crfsuite output format\n",
    "    # every line is just a tag by itself, blank lines separating sentences\n",
    "    predtags = read_tags_file(predicted_tags_file)\n",
    "\n",
    "    # commented out code for a different prediction format\n",
    "    # tokens_and_tags = read_tokens_tags_file(predicted_tags_file)\n",
    "    # predtags = [tags for tokens,tags in tokens_and_tags]\n",
    "\n",
    "    assert len(goldseqs) == len(predtags)\n",
    "\n",
    "    print(\"Span-level NER evaluation\")\n",
    "    # print \"Evaluation including NER types\"\n",
    "    # evaluate_taggings( list(zip(goldseqs, predtags)) )\n",
    "    # print \"Evaluation without types (is the span a name or not?)\"\n",
    "    evaluate_taggings( list(zip(goldseqs, predtags)), ignore_labels=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/leon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import eli5\n",
    "import scipy\n",
    "import tqdm\n",
    "import copy\n",
    "nltk.download('wordnet')\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "START1 = 'START1'\n",
    "START2 = 'START2'\n",
    "END1 = 'END1'\n",
    "END2 = 'END2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(file, isTrain=False):\n",
    "    f=open(file, 'r')\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    label = []\n",
    "    for line in f:\n",
    "        if len(line) is 1:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            labels.append(label)\n",
    "            label = []\n",
    "        else:\n",
    "            sentence.append(line.split()[0])\n",
    "            \n",
    "            # get the label if is train/dev\n",
    "            if isTrain:\n",
    "                label.append(line.split()[1])\n",
    "    \n",
    "    \n",
    "    # extract features\n",
    "    train_X = extract_feature_crf(sentences)\n",
    "    \n",
    "    return train_X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordClass(word):\n",
    "    res = re.match(r'[a-z]+', word)\n",
    "    if res is not None and res.span()[0] is 0 and res.span()[1] is len(word):\n",
    "        return 'LOWERCASE'\n",
    "\n",
    "    res = re.match(r'[A-Z]+', word)\n",
    "    if res is not None and res.span()[0] is 0 and res.span()[1] is len(word):\n",
    "        return 'allCaps'\n",
    "\n",
    "    res = re.match(r'[A-Z][a-z]*', word)\n",
    "    if res is not None and res.span()[0] is 0 and res.span()[1] is len(word):\n",
    "        return 'initCap'\n",
    "    \n",
    "    res = re.match(r'[a-z]*[A-Z]+[a-z]*', word)\n",
    "    if res is not None and res.span()[0] is 0 and res.span()[1] is len(word):\n",
    "        return 'containCap'\n",
    "\n",
    "    word = word.lower()\n",
    "    res = re.match(r'[a-z]*[0-9]+[a-z]*', word)\n",
    "    if res is not None and res.span()[0] is 0 and res.span()[1] is len(word):\n",
    "        return 'containsDigit'\n",
    "\n",
    "    return 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_crf(sentences):\n",
    "    train_features = []\n",
    "    for sentence in sentences:\n",
    "        tag_list = [START1, START2] + list(map(lambda x:x[1], nltk.pos_tag(sentence))) + [END1, END2]\n",
    "        sentence = [START1, START2] + sentence + [END1, END2]\n",
    "\n",
    "        sen_dict = []\n",
    "        for i in range(2, len(sentence)-2):\n",
    "\n",
    "            feature_dict = {}\n",
    "            cur_info = word_process(sentence[i], tag_list[i], 'cur', feature_dict)\n",
    "            prev_info = word_process(sentence[i-1], tag_list[i-1], 'prev', feature_dict)\n",
    "#             prev_prev_info = word_process(sentence[i-2], tag_list[i-2], 'prev-prev', feature_dict)\n",
    "            next_info = word_process(sentence[i+1], tag_list[i+1], 'next', feature_dict)\n",
    "#             next_next_info = word_process(sentence[i+2], tag_list[i+2], 'next-next', feature_dict)\n",
    "            \n",
    "            # more customized feature\n",
    "            word = sentence[i]\n",
    "            feature_dict['pre-1'] = word[:1]\n",
    "            feature_dict['pre-2'] = word[:2]\n",
    "            feature_dict['pre-3'] = word[:3]\n",
    "            feature_dict['pre-4'] = word[:4]\n",
    "            \n",
    "            feature_dict['post-1'] = word[-1:]\n",
    "            feature_dict['post-2'] = word[-2:]\n",
    "            feature_dict['post-3'] = word[-3:]\n",
    "            feature_dict['post-4'] = word[-4:]\n",
    "\n",
    "            sen_dict.append(feature_dict)\n",
    "        train_features.append(sen_dict)\n",
    "        \n",
    "    return train_features\n",
    "\n",
    "def extract_feature_log(train_X_crf, labels=None):\n",
    "    \n",
    "    features = []\n",
    "    for sen in train_X_crf:\n",
    "        for word in sen:\n",
    "            temp = []\n",
    "            for (key, item) in word.items():\n",
    "                temp.append(item)\n",
    "            features.append(temp)\n",
    "            \n",
    "    Y = []\n",
    "    if labels is not None:\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(len(labels[i])):\n",
    "                for k in range(len(labels[i][j])):\n",
    "                    if labels[i][j][k] == 'O':\n",
    "                        Y.append(0)\n",
    "                    else:\n",
    "                        Y.append(1)\n",
    "                        \n",
    "    return features, np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_process(word, pos_tag, prefix, feature_dict):\n",
    "    lemma = LEMMATIZER.lemmatize(word)\n",
    "    lemma_key = prefix+'-lemma'\n",
    "    pos_tag_key = prefix+'-pos-tag'\n",
    "    word_class_key = prefix+'-word-class'\n",
    "    \n",
    "    \n",
    "    _dict = {}\n",
    "#     _dict[lemma_key] = lemma\n",
    "    _dict[pos_tag_key] = pos_tag\n",
    "    _dict[word_class_key] = get_wordClass(word)\n",
    "    _dict[prefix+'-word'] = word\n",
    "#     _dict[prefix+'-len'] = len(word)\n",
    "\n",
    "#     _dict[prefix+'-pre-1'] = word[:1]\n",
    "#     _dict[prefix+'-pre-2'] = word[:2]\n",
    "#     _dict[prefix+'-pre-3'] = word[:3]\n",
    "#     _dict[prefix+'-pre-4'] = word[:4]\n",
    "\n",
    "#     _dict[prefix+'-post-1'] = word[-1:]\n",
    "#     _dict[prefix+'-post-2'] = word[-2:]\n",
    "#     _dict[prefix+'-post-3'] = word[-3:]\n",
    "#     _dict[prefix+'-post-4'] = word[-4:]\n",
    "    feature_dict.update(_dict)\n",
    "    return _dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write2file(file_path, predict):\n",
    "    fh = open(file_path, \"w\", encoding='utf-8')\n",
    "\n",
    "    for sen in predict:\n",
    "        for tag in sen:\n",
    "            fh.write(tag+'\\n')\n",
    "        fh.write('\\n')\n",
    "\n",
    "    fh.close()\n",
    "\n",
    "def convert_t0_obi(res):\n",
    "    \n",
    "    arr = ['O', 'B', 'I']\n",
    "    new_res = []\n",
    "    for i in range(len(res)):\n",
    "        if i == 0:\n",
    "            new_res.append(arr[res[i]])\n",
    "        else:\n",
    "            if (new_res[-1] == 'B' or new_res[-1] == 'I') and res[i] == 1:\n",
    "                new_res.append('I')\n",
    "            else:\n",
    "                new_res.append(arr[res[i]])\n",
    "    return new_res\n",
    "\n",
    "def convertLog2Crf(predict_log, train_crf):\n",
    "    output = []\n",
    "    index = 0\n",
    "    \n",
    "    for i in range(len(train_crf)):\n",
    "        temp = []\n",
    "        for j in range(len(train_crf[i])):\n",
    "            temp.append(predict_log[index])\n",
    "            index += 1\n",
    "        output.append(temp)\n",
    "        \n",
    "    return output\n",
    "\n",
    "def ensemble_union(results):\n",
    "    \n",
    "    _len = len(result)\n",
    "    result = results[0] \n",
    "    output = copy.deepcopy(result)\n",
    "\n",
    "    for r in results:\n",
    "        assert len(r) == _len\n",
    "    \n",
    "    for k in range(len(results)):\n",
    "        for i in range(_len):\n",
    "            for j in range(len(result[i])):\n",
    "                if results[k][i][j] is not \"O\" and output[i][j] is \"O\":\n",
    "                    output[i][j] = results[k][i][j]\n",
    "                        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_crf, train_y_crf = data_loader('data/train/train.txt', isTrain=True)\n",
    "dev_X_crf, dev_y_crf = data_loader('data/dev/dev.txt', isTrain=True)\n",
    "test_X_crf, _ = data_loader('data/test/test.nolabels.txt', isTrain=False)\n",
    "\n",
    "# get log features\n",
    "train_X_log, train_y_log = extract_feature_log(train_X_crf, labels=train_y_crf)\n",
    "dev_X_log, dev_y_log = extract_feature_log(dev_X_crf, labels=dev_y_crf)\n",
    "test_X_log, _ = extract_feature_log(test_X_crf)\n",
    "\n",
    "X_dataframe = pd.DataFrame(np.array(train_X_log + dev_X_log + test_X_log))\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "one_hot_x_training = vectorizer.fit_transform(X_dataframe.to_dict(\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x107601 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_x_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression(C=1e7)\n",
    "logistic.fit(one_hot_x_training[0:len(train_X_log)], train_y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predict_log_flat = convert_t0_obi(logistic.predict(\n",
    "    one_hot_x_training[len(train_X_log) : len(train_X_log) + len(dev_X_log)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_predict_log = convertLog2Crf(dev_predict_log_flat, dev_X_crf)\n",
    "print (evaluate_taggings(list(zip(dev_y_crf, dev_predict_log)), ignore_labels=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0,\n",
    "    c2=0.0005,\n",
    "    max_iterations=1000,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf.fit(train_X_crf, train_y_crf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F = 0.9943,  Prec = 0.9966 (1483/1488),  Rec = 0.9920 (1483/1495)\n",
      "(2394 sentences, 46469 tokens, 1495 gold spans, 1488 predicted spans)\n",
      "CPU times: user 739 ms, sys: 7.46 ms, total: 746 ms\n",
      "Wall time: 750 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_predict = crf.predict(train_X_crf)\n",
    "evaluate_taggings(list(zip(train_y_crf, train_predict)), ignore_labels=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F = 0.4973,  Prec = 0.6436 (186/289),  Rec = 0.4052 (186/459)\n",
      "(959 sentences, 13360 tokens, 459 gold spans, 289 predicted spans)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49732620320855614"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = crf.predict(dev_X_crf)\n",
    "evaluate_taggings(list(zip(dev_y_crf, predict)), ignore_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_output = ensemble_union([predict, dev_predict_log])\n",
    "print (evaluate_taggings(list(zip(dev_y_crf, en_output)), ignore_labels=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¢\n"
     ]
    }
   ],
   "source": [
    "print (dev_X_crf[0][10]['cur-word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-level NER evaluation\n"
     ]
    }
   ],
   "source": [
    "write2file('dev_predict.txt', predict)\n",
    "evaluate_tagging_file('data/dev/dev.txt', 'dev_predict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "write2file('results/test_predictions.out', crf.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "            \n",
       "                \n",
       "                \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=1\n",
       "    \n",
       "</b>\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
       "                    Weight<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +20.739\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x76947\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.55%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.934\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x74773\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.66%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.771\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x105829\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.69%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.731\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x82143\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.88%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +19.449\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x95487\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 81.87%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +18.028\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x98049\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.13%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +17.662\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x105431\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.35%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +17.344\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x77871\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.95%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.508\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x79936\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.98%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +16.465\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        x94309\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 82.98%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 8922 more positive &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
       "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
       "                    <i>&hellip; 48173 more negative &hellip;</i>\n",
       "                </td>\n",
       "            </tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "            \n",
       "        \n",
       "\n",
       "        \n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5.show_weights(logistic, top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
